{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following notes were taken during lecture 04/10/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In review, consider the following energy equation as it reacts to $\\alpha$\n",
    "\n",
    "$$E(Y,Z) = ||Y-WZ||^2 + \\alpha \\sum_i |Z_i|$$\n",
    "\n",
    "In effect, trying to recreate a datapoint as a weighted sum of vectors.\n",
    "\n",
    "The inference algorithm tries to find\n",
    "\n",
    "$$Z^* = argmin_z\\ E(Y,Z)$$\n",
    "\n",
    "There is an algorithm which accomplishes this call (isTA) iterative shrinkage and thresholding alg.\n",
    "\n",
    "$$z(T+1) = shrink \\big[ W^TY+ \\eta S Z_{(k)}$$\n",
    "\n",
    "S is matrix computed from W with constant (later)\n",
    "Shrink is point-wise non-linearity /---/\n",
    "\n",
    "Why not PCA?  Z may not have same same D as Y.  **This is main advantage of sparce coding**.\n",
    "\n",
    "To learn W matrix, apply (effectively) SGD:\n",
    "\n",
    "$$ W \\leftarrow W + \\eta (Y -WZ) Z^T $$\n",
    "\n",
    "$\\eta$ is stepsize\n",
    "\n",
    "Optimize W s.t. minimize reconstruction error using $argmin$.\n",
    "\n",
    "Need to be careful, there is L1 criterion on Z.  Normalize columns of W so it has all unique norms.  This is effectively projecting onto hypersphere.\n",
    "\n",
    "$$ W \\leftarrow norm\\_columns(W) $$\n",
    "\n",
    "These two functions are the beating heart of sparse coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Shoutout to Julien Mairal, SPAMS http://spams-devel.gforge.inria.fr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse coding learns a basis.  Looking at MNIST example, we are in fact computing shadow-looking basis images in columns of the W matrix.  Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kavukcuoglu, Ranzato, LeCun, rejected by every conference, 2008-2009\n",
    "\n",
    "Generative model for sparse encoder.  We are training it to identity function ONLY ON SAMPLE WE PROVIDE, not general image.  We achieve this with sparsity factor.\n",
    "\n",
    "> Z can be optimized using \"Target Prop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy function for inference\n",
    "\n",
    "arXiv:1010.3467 Predictive Sparse Decomposition\n",
    "\n",
    "## Better idea: FISTA\n",
    "\n",
    "Gregor & LeCun, ICML, 2010, Bronstein et al. ICML, 2012, Rolfe & LeCun ICLR 2013\n",
    "\n",
    "$$\n",
    "Z(t+1) =\n",
    "    \\texttt{Shrinkage}_{\\lambda/L} \\big[\n",
    "        Z(t) - \\frac{1}{L}W_d^T(W_dZ(t)-Y)\n",
    "    \\big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z(t+1) =\n",
    "    \\texttt{Shrinkage}_{\\lambda/L} \\big[\n",
    "        W_e^TY + SZ(t)\\big]; W_e = \\frac{1}{L}W_d ...\n",
    "$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we are using machine learning to improve inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithms to train iterative alg for fast inference procedure\n",
    "\n",
    "Same as before, but now encoder is recurrent network.\n",
    "\n",
    "In experiments, LISTA outperforms FISTA by lots!  This idea has really taken off in the last year\n",
    "\n",
    "Discriminative REcurrent Sparse Auto-Encoder (DrSAE, Rolfe & LeCun ICLR 2013)\n",
    "\n",
    "- Rectified linear units\n",
    "- Classification loss: cross-entropy\n",
    "- Reconstruction loss: squared error\n",
    "- Sparsity penalty: L1 norm of hidden layer\n",
    "- Rows of $W_d$ and columns of $W_e$ constrained in unit sphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Q: Is one-shot learning special case of sparse learning?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Sparse Coding\n",
    "\n",
    "Regular sparse coding:\n",
    "\n",
    "Convolutional S.C.:\n",
    "$$\n",
    "E(Y,Z) =\n",
    "    ||Y - \\sum_k W_k * Z_k ...\n",
    "$$\n",
    "\n",
    "Sometimes called \"deconvolutional network\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage: in patch learning, we need copy of every filter (oriented edges) at every position in patch.  With convolution operator, we can learn more complex things!\n",
    "\n",
    "Filters and Basis functions obtained with 1,2,4,8,16,32,64 filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yann idea!\n",
    "\n",
    "1. train first layer using PSD\n",
    "2. Discard reconstruction, use encoder and absolute value as feature extractor (treat L1 norm as ReLU)\n",
    "3. train the second layer using PSD\n",
    "4. ... continue as deep as needed\n",
    "\n",
    "This helps if low samples or low labels.  Unsupervised pre-training!  Very cool!  Kavukcuoglu et al. NIPS 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Invariant Features with L2 Group Sparsity\n",
    "\n",
    "- Number of feature ...\n",
    "- Pools tend to regroup similar features\n",
    "\n",
    "$$\n",
    "E(Y,Z) = \n",
    "    ||Y-W_d||^2 + Z-g_e(W_e,Y)||^2 + \\sum_j\\sqrt{\\sum_{k \\in P_j} Z_k^2}\n",
    "$$\n",
    "\n",
    "I want to minimize sum of these guys, during training, minimize number of non-zero components in vector.  Activate smallest number of units necessary!  Pooling over a region, I don't care how many are active, just want minimal number of active units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups are local in a 2D Topographic Map\n",
    "\n",
    "Convolution kernels that result from this training have \"fingerprint\" quality.  Within group, as many as possible should fire.  Between groups, as few should fire as possible.  Like in the brain, neighboring neurons detect similar features.\n",
    "\n",
    "Simple algorithms, simple constraints result in beautiful emergence of patterns appearing in nature (human brain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting Temporal Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scattering transform, Stephane Mallat, Joan Bruna\n",
    "\n",
    "> Mark Tygert has a publication on Nerual Computation (math just. for complex number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Q:Does PyTorch support complex numbers?  Why not?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant Features through Temporal Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In shape example, we have 2 explanitory factors (shape, size).  We want a network to learn there are two independent explanatory factors.\n",
    "\n",
    "Karol Gregor et al. (at NYU, now at DeepMind)\n",
    "\n",
    "Mapping Units, (Hinton 1981), capsules (Hinton 2011)\n",
    "\n",
    "If you learn things independently, you can choose which to care about!  Think grabbing an object.  We don't care what it is, we just want to grasp it!\n",
    "\n",
    "How could we do this?\n",
    "\n",
    "One idea is temp. constistency.  Think cue that image is same target in different positions.\n",
    "\n",
    "Assumption: between two frames, shape won't change much, position won't change much.  So given shape is similar, we can exploit consistency:\n",
    "\n",
    "Idea Karol had: system reconstructs successive frames that are observed, use two paths:\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "1. Represent identity (same for all three frames)\n",
    "2. Represent position (different for all three frames)\n",
    "\n",
    "Combine multiplicitivly (term-by-term).  Plug that in to reconstruction network to recreate image.\n",
    "\n",
    "Training: show 3 successive frames.  Through sparse-coding/inference ...\n",
    "\n",
    "Why good idea?  You might imagine role of edger is \"edge exists, not sure where...\".  Role of placer is \"something is here, not sure what\".  What*where is edge/orientation!\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Same thing, but backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Auto-Encoder with \"Slow Feature\" Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goroshin et al. ICCV 2015 ArXiv: 1412.6056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating Supervised & Unsupervised Learning with Stacked What-Where Auto-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a.k.a. ladder networks (hey, we tried this!)*\n",
    "\n",
    "A ladder network is a FF-network paired with a reverse network of the same topology.  There are typically some lateral connections (indices) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised and Unsupervised in One Learning Rule?\n",
    "\n",
    "Problem: need a one-to-many mapping (not a function!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: keep the complementary information around\n",
    "\n",
    "- So that the generative path is a function\n",
    "\n",
    "What-Where Auto-Encoder\n",
    "\n",
    "- [Ranzato 2007], [Gregor 2011], Hinton's Capsules\n",
    "\n",
    "- Very old ideas by Hinton & Zemel, von der Malsburg and others about separating identity from instantiation parameters.\n",
    "\n",
    "- Propagate \"where\" as indices of maxes.  \"what\" is the invariant code learned by feed-forward layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A funny kind of pooling/unpooling\n",
    "\n",
    "To make this all work with SGD we make pooling/unpooling smooth:\n",
    "\n",
    "$$\n",
    "m_k =\n",
    "    \\sum_{N_K}z(f,x,u)\n",
    "        \\frac\n",
    "            {e^{ \\beta z (f,x,y) }}\n",
    "            {\\sum_{N_k}e^{\\beta z(f',x',y')}}\n",
    "\\approx {max}_{N_k} z(f,x,y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWWAE on adversarial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
