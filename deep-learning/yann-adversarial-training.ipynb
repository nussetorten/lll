{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following was recorded 4/17/2017 at NYU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common sense is the most instinctual thing to teach a computer but it is the hardest thing to codify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obstacles to Progress in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines need to know how the world works.  This is the WMRL from Princeton lecture.\n",
    "\n",
    "Machines need to:\n",
    "\n",
    "- Percieve a subset of the world\n",
    "- Update and retain state estimates\n",
    "- Reason and plan according to current and estimated state\n",
    "\n",
    "Intelligence *could* be defined as **Perception + Predictive Model + Memory + Reasoning & Planning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://g.gravizo.com/svg?\n",
    "  digraph G {\n",
    "    aize =\"4,4\";\n",
    "    world -> perception [label=\"percepts/observations\"];\n",
    "    perception -> agent;\n",
    "    agent -> objective [label=\"state\"];\n",
    "    agent -> world [label=\"actions/outputs\"];\n",
    "    objective -> [label=\"cost\"]\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Common Sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is an ongoing approach that has created a set of logic and rules (*G*ood *O*ld *F*ashion *AI*)\n",
    "> and has persisted for 30 years.\n",
    "\n",
    "Whatever common sense is, computers do not have it.\n",
    "\n",
    "Example: what does pronoun refer to in image caption?\n",
    "\n",
    "\"The trophy doesn't fit in the suitcase bercause it's too [large,small]\" (winograd schema)\n",
    "\n",
    "\"Top picked up his back and left the room\" implies Tom opened a door, opened the door, and walks through the door.  Tom didn't evaporate onto the Starship Enterprise.  Possible, but not likely.\n",
    "\n",
    "Without knowledge of the world (common sense) there's no way a machine would know this *obvious* fact.\n",
    "\n",
    "You can reconstruct a scene from a simple sentence, but a machine cannot.\n",
    "\n",
    "Talking with machine is ultimately frustrating unless you stay within its domain.  Because you have to teach it everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Sense is our ability to fill in the blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "- Infer state of world from partial information\n",
    "- Infer state of world from past and present\n",
    "- Infer past events from the present state\n",
    "\n",
    "## Prediction\n",
    "\n",
    "- Filing in the visual field at the retinal blind spot\n",
    "- Filling in occluded images\n",
    "- Filling in missing segments in text, missing words in speech\n",
    "- Predicting the consequences of our actions\n",
    "- Predicting the sequence of actions leading to a result\n",
    "\n",
    "Predicting any part of the past, present, or future percepts from whatever information is available.  That's what predictive learning is!  But really, that's what many people mean by unserpervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Necessity of Unsupervised Learning / Predictive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples required to train a large learning machine (for any task) depends on the amount of information that we ask it to predict.  The more you ask the machine, the larger it can be.\n",
    "\n",
    "> The brain has about $10^4$ synapses and we only live for about $10^9$ seconds.  So we have a lot more parameters than data.  This motivates the idea that we must do a lot of unsupervised learning since the perrceptiual input (including proprioception) is the only place we can get $10^5$ dimensions of constraint per second\n",
    ">\n",
    "> Geofry Hinton (https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyjogf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much information does the Machine Predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Pure\" Reinforcement Learning (a ew bits for some samples)\n",
    "\n",
    "- The machine predicts a scalar reward given once in a while\n",
    "- A few bits for some samples\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "- The machine predicts a category or a few numbers for each input\n",
    "- Predicting human-supplied data\n",
    "- 10 $\\rightarrow$ 10000 bits per sample\n",
    "\n",
    "## Unsupervised/Predictive Learning\n",
    "\n",
    "- The machine predicts any part of its input for any observed part\n",
    "- Predicts future frames in videos\n",
    "- Millions of bits per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: is the internet a rich enough environment that a complete AI (w/ common sense) could emerge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " VisDoom 2016 (ICLR 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sutton's Dyna Architecture: \"imagine before acting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://g.gravizo.com/svg?\n",
    "  digraph G {\n",
    "    aize =\"4,4\";\n",
    "    input -> black_box [label=\"y\"];\n",
    "    input -> \"G[X,Y]\" [label=\"x\"];\n",
    "    \"G[X,Y]\" -> black_box [label=\"~y\"];\n",
    "    black_box -> \"E[X,Y]\";\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diagram, `black_box` is non-differentiable.  Black-box optimization is used to solve.  There exists an algorithm called REINFORCE by Williams to solve this.  In Reinforcement Learning, `y` doesn't even come from the input distribution!  It is some other value (cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-critic is an interesting thing.  Look it up.  $\\tilde{E}_W(X,\\tilde{Y}), E_W(X,\\tilde{Y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable cost function $C$ so that it gives low cost to observed Y from teacher.  Imitation learning or Inverse Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Model-Based Optimal Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simulate the world (the plant) with an initial control sequence\n",
    "- Adjust the control sequence to optimize the objective through gradient desent\n",
    "- Backprop through time was invented by control theorists in the late 1950s\n",
    "  - Its sometimes called the adjoint state method\n",
    "  - Athans & Falb 1966, Bryson & Ho 1969\n",
    "  \n",
    "> This kind of control is used by SpaceX and NASA to model rocket flight!\n",
    "\n",
    "> Applied Optimal Control (basically back-prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of Back Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is sequence of modules with weights\n",
    "\n",
    "$$Z_{k}=F_k(Z_{k-1},W_k)$$\n",
    "\n",
    "To derive back-prop, observe Lagrangian:\n",
    "\n",
    "$$L(Z_{...}^i,Y_{...}^i,W_{...})=C(Z_l^i,Y^i) + \\sum_k \\lambda_k^T(Z_k - F_k(Z_{k-1},W_k))$$\n",
    "\n",
    "$$L(W) = \\sum_i L(Z_{...}^i,Y_{...}^i,W_{...})$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\lambda_k} = 0 \\rightarrow Z_k = F_k(Z_{k-1}, W_k)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Z_k} = 0 \\rightarrow \\lambda_k = \\frac{\\partial F_{k-1}}{\\partial}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of Intelligent System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essense of intelligence is the ability to predict.  To plan ahead, we simulate the world.  The action taken minimizes the predicted cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Model: Entity RNN + Internactions (Henaff, Whitney, LeCun 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Based Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our class talking about multiple correct predictions, don't want to call them all false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arXiv:1609.03126 Energy-Based GAN\n",
    "\n",
    "arXiv:1609.03126 Jake Zhou is first author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
